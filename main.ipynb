{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langdetect import detect, LangDetectException\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Twitter_Data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def remove_url(text):\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "def remove_mentions(text):\n",
    "    return re.sub(r'@\\w+', '', text)\n",
    "def remove_hashtags(text):\n",
    "    return   re.sub(r'#\\w+', '', text)\n",
    "def remove_special_characters_and_numbers(text):\n",
    "    return  re.sub(r'[^\\w\\s]', '', text)\n",
    "def remove_emojis(text):\n",
    "    return emoji.demojize(text)\n",
    "def remove_mult_spaces(text):\n",
    "    return re.sub(r\"\\s\\s+\", \" \", text)\n",
    "def lower_case(text):\n",
    "    return  text.lower()\n",
    "def lower_case(text):\n",
    "    return  text.translate(str.maketrans('', '', punctuation))\n",
    "def lemmatize(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return  \" \".join(lemmatized_tokens)\n",
    "def remove_short_tweets(text,min_words=5):\n",
    "    words = text.split()\n",
    "    return text if len(words)>min_words else ''\n",
    "def remove_white_spaces(text):\n",
    "    return \" \".join(text.split())\n",
    "def filter_non_english(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except LangDetectException:\n",
    "        lang = \"unknown\"\n",
    "    return text if lang == \"en\" else \"\"\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "    text=remove_url(text)\n",
    "    text=remove_mentions(text)\n",
    "    text=remove_hashtags(text)\n",
    "    text=remove_special_characters_and_numbers(text)\n",
    "    text=remove_emojis(text)\n",
    "    text=remove_mult_spaces(text)\n",
    "    text=lower_case(text)\n",
    "    text=lower_case(text)\n",
    "    text=lemmatize(text)\n",
    "    text=remove_short_tweets(text,min_words=5)\n",
    "    text=remove_white_spaces(text)\n",
    "    '''text=remove_stopwords(text)\n",
    "    text=filter_non_english(text)'''\n",
    "    return text \n",
    "data['clean_text'] = data['clean_text'].apply(clean_data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset='clean_text', keep='first', inplace=True)\n",
    "duplicates_mask = data.duplicated(subset='clean_text', keep=False)\n",
    "num_duplicates = duplicates_mask.sum()\n",
    "num_duplicates \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['category'], inplace=True)\n",
    "data['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_mask = data.isnull().sum()\n",
    "missing_values_mask \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(how='any', inplace=True)\n",
    "missing_values_mask = data.isnull().sum()\n",
    "missing_values_mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counts = data['category'].value_counts()\n",
    "target_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['category'] =data['category'].replace({0:'neutral',1:'positive',-1:'negative'})\n",
    "data['category'] =data['category'].replace({'neutral':0,'positive':2,'negative':1})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counts = data['category'].value_counts()\n",
    "target_counts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "# Tokenize the text data into words for each tweet\n",
    "tokenized_text = [word_tokenize(text) for text in data['clean_text']]\n",
    "\n",
    "# Create a vocabulary set to store unique words from the tokenized text\n",
    "vocab = set(word for tokens in tokenized_text for word in tokens)\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "\n",
    "# Create a dictionary to store the occurrence count of each word\n",
    "word_occurrence = {}\n",
    "\n",
    "# Count the occurrences of each word in the tokenized text\n",
    "for tokens in tokenized_text:\n",
    "    for word in tokens:\n",
    "        if word in word_occurrence:\n",
    "            word_occurrence[word] += 1\n",
    "            if word_occurrence[word] > 10000:\n",
    "                word_occurrence[word] = 10000\n",
    "        else:\n",
    "            word_occurrence[word] = 1\n",
    "\n",
    "# Replace the words in tokenized text with their occurrence counts\n",
    "tokenized_text_with_counts = [[word_occurrence[word] for word in tokens] for tokens in tokenized_text]\n",
    "\n",
    "\n",
    "# Convert the tokenized text with word counts into torch tensors\n",
    "tensor_sequences = [torch.tensor(sequence) for sequence in tokenized_text_with_counts]\n",
    "\n",
    "# Pad the tensor sequences to create equal-length sequences for batch processing\n",
    "padded_sequences = pad_sequence(tensor_sequences, batch_first=True)\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_occurrence = max(word_occurrence.values())\n",
    "print(\"Maximum Occurrence Count:\", max_occurrence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(padded_sequences.shape)\n",
    "X=padded_sequences\n",
    "y=data['category'].values\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    " \n",
    "# Train Word2Vec model on the tokenized_text data\n",
    "model_w2v = Word2Vec(sentences=tokenized_text, vector_size=200, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Create a dictionary to store word embeddings (word vectors) for each word in the vocabulary\n",
    "word_embeddings = {word: model_w2v.wv[word] for word in model_w2v.wv.key_to_index}\n",
    "\n",
    "# Create a list of word embeddings for all words in the vocabulary\n",
    "word_vector_list = [word_embeddings[word] for word in model_w2v.wv.key_to_index]\n",
    "\n",
    "# Convert the list of word embeddings into a numpy array\n",
    "word_embeddings_np = np.stack(word_vector_list)\n",
    "\n",
    "# Print the shape of the word embeddings numpy array\n",
    "print(\"Shape of word embeddings array:\", word_embeddings_np.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Imbalanced Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:60000], y[:60000], test_size=0.2, random_state=42)\n",
    "oversampler = RandomOverSampler()\n",
    "X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert X_train and y_train to PyTorch tensors\n",
    "train_data_tensor = torch.tensor(X_train)   \n",
    "train_label_tensor = torch.tensor(y_train) \n",
    "\n",
    "# Convert the data and labels to float32 data type\n",
    "train_data_tensor = train_data_tensor.to(torch.float32)\n",
    "train_label_tensor = train_label_tensor.to(torch.float32) \n",
    "\n",
    "# Create a PyTorch TensorDataset using the training data and labels\n",
    "train_dataset = TensorDataset(train_data_tensor, train_label_tensor)\n",
    "\n",
    "# Convert X_test and y_test to PyTorch tensors and set their data type to float32\n",
    "test_data_tensor = torch.tensor(X_test).to(torch.float32) \n",
    "test_label_tensor = torch.tensor(y_test).to(torch.float32) \n",
    "\n",
    "# Create a PyTorch TensorDataset using the test data and labels\n",
    "test_dataset = TensorDataset(test_data_tensor, test_label_tensor)\n",
    "\n",
    "# Create a DataLoader for training data with batch_size=32 and shuffle the data during training\n",
    "train_loader = DataLoader(train_dataset, batch_size=200, shuffle=True, drop_last=True)\n",
    "\n",
    "# Create a DataLoader for test data with batch_size=32 and shuffle the data during testing\n",
    "test_loader = DataLoader(test_dataset, batch_size=200, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the input data (features) from the training dataset\n",
    "input_data_shape = train_dataset.tensors[0].shape\n",
    "\n",
    "# Get the shape of the labels from the training dataset\n",
    "label_shape = train_dataset.tensors[1].shape\n",
    "\n",
    "# Print the shape of the input data (features)\n",
    "print(\"Input data shape:\", input_data_shape)\n",
    "\n",
    "# Print the shape of the labels\n",
    "print(\"Label shape:\", label_shape)\n",
    "\n",
    "# Get the data type of the input data (features) from the training dataset\n",
    "input_data_type = train_dataset.tensors[0].dtype\n",
    "\n",
    "# Get the data type of the labels from the training dataset\n",
    "label_data_type = train_dataset.tensors[1].dtype\n",
    "\n",
    "# Print the data type of the input data (features)\n",
    "print(\"Input data type:\", input_data_type)\n",
    "\n",
    "# Print the data type of the labels\n",
    "print(\"Label data type:\", label_data_type)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building ,Training and Evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking Lstms with attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim, is_bidirectional):\n",
    "        super(Attention, self).__init__()\n",
    "        self.is_bidirectional = is_bidirectional\n",
    "        # The attention linear layer which transforms the input data to the hidden space\n",
    "        self.attn = nn.Linear(hidden_dim * (4 if is_bidirectional else 2), hidden_dim * (2 if is_bidirectional else 1))\n",
    "        # The linear layer that calculates the attention scores\n",
    "        self.v = nn.Linear(hidden_dim * (2 if is_bidirectional else 1), 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        # Concatenate the last two hidden states in case of a bidirectional LSTM\n",
    "        if self.is_bidirectional:\n",
    "            hidden = torch.cat((hidden[-2], hidden[-1]), dim=-1)\n",
    "        else:\n",
    "            hidden = hidden[-1]\n",
    "        # Repeat the hidden state across the sequence length\n",
    "        hidden_repeated = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        # Calculate attention weights\n",
    "        attn_weights = torch.tanh(self.attn(torch.cat((hidden_repeated, encoder_outputs), dim=2)))\n",
    "        # Compute attention scores\n",
    "        attn_weights = self.v(attn_weights).squeeze(2)\n",
    "        # Apply softmax to get valid probabilities\n",
    "        return nn.functional.softmax(attn_weights, dim=1)\n",
    "\n",
    "\n",
    "class LSTM_Sentiment_Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, lstm_layers, dropout, is_bidirectional):\n",
    "        super(LSTM_Sentiment_Classifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = lstm_layers\n",
    "        self.is_bidirectional = is_bidirectional\n",
    "\n",
    "        # The Embedding layer that converts input words to embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # LSTM layer which processes the embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, lstm_layers, batch_first=True, bidirectional=is_bidirectional)\n",
    "        # Attention layer to compute the context vector\n",
    "        self.attention = Attention(hidden_dim, is_bidirectional)\n",
    "        # Fully connected layer which classifies the context vector into classes\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if is_bidirectional else 1), num_classes)\n",
    "        # Apply LogSoftmax to outputs for numerical stability\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        # Dropout layer for regularisation\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Transform words to embeddings\n",
    "        embedded = self.embedding(x)\n",
    "        # Pass embeddings to LSTM\n",
    "        out, hidden = self.lstm(embedded, hidden)\n",
    "        # Calculate attention weights\n",
    "        attn_weights = self.attention(hidden[0], out)\n",
    "        # Calculate context vector by taking the weighted sum of LSTM outputs\n",
    "        context = attn_weights.unsqueeze(1).bmm(out).squeeze(1)\n",
    "        # Classify the context vector\n",
    "        out = self.softmax(self.fc(context))\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Factor determines the size of hidden states depending on bidirectionality\n",
    "        factor = 2 if self.is_bidirectional else 1\n",
    "        # Initial hidden and cell states are zero\n",
    "        h0 = torch.zeros(self.num_layers * factor, batch_size, self.hidden_dim)\n",
    "        c0 = torch.zeros(self.num_layers * factor, batch_size, self.hidden_dim)\n",
    "        hidden=(h0,c0)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of classes for classification\n",
    "input_size = 3\n",
    "\n",
    "# Define the hidden dimension for the LSTM model\n",
    "hidden_dim = 100\n",
    "\n",
    "# Define the number of LSTM layers\n",
    "lstm_layers = 2\n",
    "\n",
    "# Specify whether the LSTM model is bidirectional or not\n",
    "is_bidirectional = True\n",
    "\n",
    "# Set the learning rate for the optimizer\n",
    "lr= 4e-4\n",
    "\n",
    "# Set the dropout rate for the LSTM model\n",
    "dropout = 0.5\n",
    "\n",
    "# Set the number of training epochs\n",
    "epochs = 5\n",
    "# Check if CUDA (GPU) is available, else use CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set the size of the vocabulary (number of unique words in the dataset)\n",
    "VOCAB_SIZE = 101223\n",
    "\n",
    "# Create an instance of the LSTM Sentiment Classifier model with specified hyperparameters\n",
    "model = LSTM_Sentiment_Classifier(VOCAB_SIZE, 200, hidden_dim, input_size, lstm_layers, dropout, is_bidirectional)\n",
    "\n",
    "# Move the model to the specified device (CUDA/GPU or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize the embedding layer with the previously defined embedding matrix\n",
    "model.embedding.weight.data.copy_(torch.from_numpy(word_embeddings_np))\n",
    "\n",
    "# Allow the embedding matrix to be fine-tuned during training for better adaptation to the dataset\n",
    "model.embedding.weight.requires_grad = True\n",
    "\n",
    "# Set up the criterion (loss function) for the model's training\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Set up the optimizer for training the model's parameters\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=5e-6)\n",
    "\n",
    "# Print the model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(10):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.long() \n",
    "        labels = labels.long()\n",
    "        h = model.init_hidden(labels.size(0))\n",
    "        print(inputs.shape)\n",
    "        model.zero_grad()  \n",
    "        output, h = model(inputs,h) \n",
    "        loss = criterion(output , labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_pred_list = []\n",
    "y_test_list = []\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "            inputs = inputs.long() \n",
    "            labels = labels.long()\n",
    "            test_h = model.init_hidden(labels.size(0))\n",
    "            output, val_h = model(inputs, test_h)\n",
    "            y_pred_test = torch.argmax(output, dim=1)\n",
    "            y_pred_list.extend(y_pred_test.squeeze().tolist())\n",
    "            y_test_list.extend(labels.squeeze().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test_list, y_pred_list)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "class_labels=['neutral','negative','positive']\n",
    "cm = confusion_matrix(y_test_list, y_pred_list)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Purples\", xticklabels=class_labels,\n",
    "            yticklabels=class_labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix1.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report for Bidirectional LSTM:\")\n",
    "classification_report_lstm=classification_report(y_test_list, y_pred_list)\n",
    "print(classification_report_lstm)\n",
    "with open(\"classification_report_lstm1.txt\", \"w\") as file:\n",
    "    file.write(\"Classification Report for Bidirectional LSTM:\\n\")\n",
    "    file.write(classification_report_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"bidirectional_lstm_model1.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "# Load the pre-trained BERT tokenizer\n",
    "all_input_ids = []\n",
    "all_attention_masks = []\n",
    "\n",
    "data_sentences = data['clean_text'].tolist()\n",
    "\n",
    "\n",
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Loop through all the text sequences in data_sentences\n",
    "for text in data_sentences:\n",
    "    # Tokenize and encode each text sequence\n",
    "    encoded_text = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length',\n",
    "        max_length=64,\n",
    "        return_tensors='pt',\n",
    "        return_attention_mask=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    # Get the input IDs and attention mask tensors for the current text sequence\n",
    "    input_ids = encoded_text['input_ids']\n",
    "    attention_mask = encoded_text['attention_mask']\n",
    "    \n",
    "    # Add the input IDs and attention masks to the lists\n",
    "    all_input_ids.append(input_ids)\n",
    "    all_attention_masks.append(attention_mask)\n",
    "\n",
    "# Stack the input IDs and attention masks into tensors\n",
    "input_ids_tensor = torch.cat(all_input_ids, dim=0)\n",
    "attention_mask_tensor = torch.cat(all_attention_masks, dim=0)\n",
    "\n",
    "# Print the shape of the tensors to verify\n",
    "print(\"Input IDs Shape:\", input_ids_tensor.shape)\n",
    "print(\"Attention Mask Shape:\", attention_mask_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification,DistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "\n",
    "\n",
    "labels = torch.tensor(y[3000:7000], dtype=torch.long)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(input_ids_tensor[3000:7000], attention_mask_tensor[3000:7000], labels)\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 100\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the BERT model for sequence classification\n",
    "num_labels = len(set(y))\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for input_ids, attention_mask, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, labels in test_loader:\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accurcay: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "model.eval()\n",
    "y_test_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, labels in test_loader:\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        \n",
    "        y_test_list.extend(labels.cpu().numpy())\n",
    "        y_pred_list.extend(predicted.cpu().numpy())\n",
    "cm = confusion_matrix(y_test_list, y_pred_list)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "\n",
    "report = classification_report(y_test_list, y_pred_list)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "with open('classification_report_Bert1.txt', 'w') as file:\n",
    "    file.write(report)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    " \n",
    "cm = confusion_matrix(y_test_list, y_pred_list)\n",
    "\n",
    " \n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Purples', fmt='d', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion matrix BERT1.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming you have a CSV file named 'data.csv' with 'text' and 'label' columns\n",
    "data = pd.read_csv('Twitter_Data.csv')\n",
    "data.dropna(how='any', inplace=True)\n",
    "missing_values_mask = data.isnull().sum()\n",
    "# Preprocess the text data (e.g., lowercasing, removing stopwords, etc.)\n",
    "# For simplicity, we'll just lowercase the text in this example\n",
    "data['clean_text'] = data['clean_text'].str.lower()\n",
    "\n",
    "# Extract the features (text) and labels\n",
    "X = data['clean_text'].values\n",
    "y = data['category'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:10000], y[:10000], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the TfidfVectorizer to convert text data into numerical features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Create the SVM classifier\n",
    "svm_model = SVC()\n",
    "\n",
    "# Train the SVM model on the training data\n",
    "svm_model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = svm_model.predict(X_test_vec)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
